# ai-lib-python

**AI-Protocol å®˜æ–¹ Python è¿è¡Œæ—¶** - ç»Ÿä¸€ AI æ¨¡å‹äº¤äº’çš„è§„èŒƒ Python å®ç°ã€‚

[![PyPI Version](https://img.shields.io/pypi/v/ai-lib-python.svg)](https://pypi.org/project/ai-lib-python/)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-green.svg)](LICENSE)
[![Tests](https://github.com/hiddenpath/ai-lib-python/actions/workflows/ci.yml/badge.svg)](https://github.com/hiddenpath/ai-lib-python/actions)

## ğŸ¯ è®¾è®¡ç†å¿µ

`ai-lib-python` æ˜¯ [AI-Protocol](https://github.com/hiddenpath/ai-protocol) è§„èŒƒçš„**å®˜æ–¹ Python è¿è¡Œæ—¶**ã€‚ä½œä¸º AI-Protocol å›¢é˜Ÿç»´æŠ¤çš„è§„èŒƒ Python å®ç°ï¼Œå®ƒä½“ç°äº†æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š

> **ä¸€åˆ‡é€»è¾‘çš†ç®—å­ï¼Œä¸€åˆ‡é…ç½®çš†åè®®** (All logic is operators, all configuration is protocol)

ä¸ä¼ ç»Ÿçš„é€‚é…å™¨åº“ç¡¬ç¼–ç ç‰¹å®šæä¾›å•†é€»è¾‘ä¸åŒï¼Œ`ai-lib-python` æ˜¯ä¸€ä¸ª**åè®®é©±åŠ¨çš„è¿è¡Œæ—¶**ï¼Œæ‰§è¡Œ AI-Protocol è§„èŒƒã€‚è¿™æ„å‘³ç€ï¼š

- **é›¶ç¡¬ç¼–ç æä¾›å•†é€»è¾‘**: æ‰€æœ‰è¡Œä¸ºéƒ½ç”±åè®®æ¸…å•ï¼ˆYAML/JSON é…ç½®ï¼‰é©±åŠ¨
- **åŸºäºç®—å­çš„æ¶æ„**: é€šè¿‡å¯ç»„åˆçš„ç®—å­ï¼ˆè§£ç å™¨ â†’ é€‰æ‹©å™¨ â†’ ç´¯ç§¯å™¨ â†’ æ‰‡å‡º â†’ äº‹ä»¶æ˜ å°„å™¨ï¼‰è¿›è¡Œå¤„ç†
- **å¯çƒ­é‡è½½**: åè®®é…ç½®å¯ä»¥åœ¨ä¸é‡å¯åº”ç”¨ç¨‹åºçš„æƒ…å†µä¸‹æ›´æ–°
- **ç»Ÿä¸€æ¥å£**: å¼€å‘è€…ä¸å•ä¸€ã€ä¸€è‡´çš„ API äº¤äº’ï¼Œæ— è®ºåº•å±‚æä¾›å•†æ˜¯ä»€ä¹ˆ

## ğŸš€ å¿«é€Ÿå…¥é—¨

### åŸºç¡€ç”¨æ³•

```python
import asyncio
from ai_lib_python import AiClient, Message

async def main():
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = await AiClient.create("openai/gpt-4o")

    # ç®€å•çš„èŠå¤©è¡¥å…¨
    response = await (
        client.chat()
        .user("ä½ å¥½ï¼2+2 ç­‰äºå¤šå°‘ï¼Ÿ")
        .execute()
    )
    print(response.content)
    # è¾“å‡º: 2+2 ç­‰äº 4ã€‚

    await client.close()

asyncio.run(main())
```

## âœ¨ ç‰¹æ€§

- **åè®®é©±åŠ¨**: æ‰€æœ‰è¡Œä¸ºç”± YAML/JSON åè®®æ–‡ä»¶é©±åŠ¨
- **ç»Ÿä¸€æ¥å£**: å•ä¸€ API æ”¯æŒæ‰€æœ‰ AI æä¾›å•†ï¼ˆOpenAIã€Anthropicã€Geminiã€DeepSeek ç­‰ï¼‰
- **æµå¼ä¼˜å…ˆ**: åŸç”Ÿå¼‚æ­¥æµå¼å¤„ç†ï¼Œä½¿ç”¨ Python çš„ `async for`
- **ç±»å‹å®‰å…¨**: å®Œæ•´çš„ç±»å‹æ³¨è§£ï¼ŒåŸºäº Pydantic v2 æ¨¡å‹
- **ç”Ÿäº§å°±ç»ª**: å†…ç½®é‡è¯•ã€é™æµã€ç†”æ–­å’Œé™çº§æ”¯æŒ
- **æ˜“äºæ‰©å±•**: é€šè¿‡åè®®é…ç½®è½»æ¾æ·»åŠ æ–°æä¾›å•†
- **å¤šæ¨¡æ€æ”¯æŒ**: æ”¯æŒæ–‡æœ¬ã€å›¾åƒï¼ˆbase64/URLï¼‰å’ŒéŸ³é¢‘
- **é¥æµ‹åŠŸèƒ½**: ç»“æ„åŒ–æ—¥å¿—ã€æŒ‡æ ‡æ”¶é›†ã€åˆ†å¸ƒå¼è¿½è¸ªå’Œç”¨æˆ·åé¦ˆæ”¶é›†
- **Token è®¡æ•°**: tiktoken é›†æˆå’Œæˆæœ¬ä¼°ç®—
- **è¿æ¥æ± **: é«˜æ•ˆçš„ HTTP è¿æ¥ç®¡ç†
- **æ‰¹å¤„ç†**: å¹¶å‘æ‰§è¡Œä¸å¹¶å‘æ§åˆ¶
- **æ¨¡å‹è·¯ç”±**: æ™ºèƒ½æ¨¡å‹é€‰æ‹©ä¸è´Ÿè½½å‡è¡¡ç­–ç•¥
- **å‘é‡åµŒå…¥**: åµŒå…¥å‘é‡ç”Ÿæˆä¸å‘é‡è¿ç®—
- **ç»“æ„åŒ–è¾“å‡º**: JSON æ¨¡å¼ä¸ Schema éªŒè¯
- **å“åº”ç¼“å­˜**: å¤šåç«¯ç¼“å­˜æ”¯æŒï¼Œå¸¦ TTL ç®¡ç†
- **æ’ä»¶ç³»ç»Ÿ**: å¯æ‰©å±•çš„é’©å­å’Œä¸­é—´ä»¶æ¶æ„
- **æµå¼å–æ¶ˆ**: æµå¼æ“ä½œçš„åä½œå¼å–æ¶ˆ

## ğŸ“¦ å®‰è£…

```bash
pip install ai-lib-python
```

å®‰è£…å¯é€‰åŠŸèƒ½ï¼š

```bash
# å®Œæ•´å®‰è£…ï¼ŒåŒ…å«æ‰€æœ‰åŠŸèƒ½
pip install ai-lib-python[full]

# é¥æµ‹åŠŸèƒ½ï¼ˆOpenTelemetry é›†æˆï¼‰
pip install ai-lib-python[telemetry]

# Token è®¡æ•°ï¼ˆtiktokenï¼‰
pip install ai-lib-python[tokenizer]

# Jupyter notebook é›†æˆ
pip install ai-lib-python[jupyter]

# å¼€å‘ç¯å¢ƒ
pip install ai-lib-python[dev]
```

## ğŸ”§ é…ç½®

åº“ä¼šè‡ªåŠ¨åœ¨ä»¥ä¸‹ä½ç½®ï¼ˆæŒ‰é¡ºåºï¼‰æŸ¥æ‰¾åè®®æ¸…å•ï¼š

1. é€šè¿‡ `AI_PROTOCOL_PATH` ç¯å¢ƒå˜é‡è®¾ç½®çš„è‡ªå®šä¹‰è·¯å¾„
2. å¸¸è§çš„å¼€å‘è·¯å¾„ï¼š`ai-protocol/`ã€`../ai-protocol/`ã€`../../ai-protocol/`
3. æœ€åæ‰‹æ®µï¼šGitHub raw `hiddenpath/ai-protocol` (main)

æä¾›å•†æ¸…å•æŒ‰å‘åå…¼å®¹çš„é¡ºåºè§£æï¼š
`dist/v1/providers/<id>.json` â†’ `v1/providers/<id>.yaml`ã€‚

### æœ‰ç”¨çš„ç¯å¢ƒå˜é‡

| å˜é‡ | æè¿° | é»˜è®¤å€¼ |
|------|------|--------|
| `AI_PROTOCOL_PATH` | è‡ªå®šä¹‰åè®®ç›®å½•ï¼ˆæœ¬åœ°è·¯å¾„æˆ– GitHub URLï¼‰ | - |
| `AI_HTTP_TIMEOUT_SECS` | HTTP è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ | 60 |
| `AI_LIB_MAX_INFLIGHT` | æœ€å¤§å¹¶å‘è¯·æ±‚æ•° | 10 |
| `AI_LIB_RPS` | é€Ÿç‡é™åˆ¶ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰ | - |
| `AI_LIB_BREAKER_FAILURE_THRESHOLD` | ç†”æ–­å™¨å¤±è´¥é˜ˆå€¼ | 5 |
| `AI_LIB_BREAKER_COOLDOWN_SECS` | ç†”æ–­å™¨å†·å´ç§’æ•° | 30 |

### æä¾›å•† API å¯†é’¥

è¿è¡Œæ—¶ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– API å¯†é’¥ï¼Œæ ¼å¼ä¸ºï¼š`<PROVIDER_ID>_API_KEY`

```bash
# è®¾ç½® API å¯†é’¥
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GOOGLE_API_KEY="..."
export DEEPSEEK_API_KEY="..."
```

**æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ**ï¼šåœ¨ CI/CDã€å®¹å™¨å’Œç”Ÿäº§éƒ¨ç½²ä¸­ä½¿ç”¨ç¯å¢ƒå˜é‡ã€‚

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### æµå¼å“åº”

```python
async def stream_example():
    client = await AiClient.create("anthropic/claude-3-5-sonnet")
    
    async for event in (
        client.chat()
        .system("ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚")
        .user("è®²ä¸€ä¸ªç®€çŸ­çš„æ•…äº‹ã€‚")
        .stream()
    ):
        if event.is_content_delta:
            print(event.as_content_delta.content, end="", flush=True)
    
    print()  # æœ«å°¾æ¢è¡Œ
    await client.close()
```

### ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨

```python
from ai_lib_python import Message

messages = [
    Message.system("ä½ æ˜¯ä¸€ä¸ª Python ä¸“å®¶ã€‚"),
    Message.user("å¦‚ä½•åœ¨ Python ä¸­è¯»å–æ–‡ä»¶ï¼Ÿ"),
]

response = await (
    client.chat()
    .messages(messages)
    .temperature(0.7)
    .max_tokens(1024)
    .execute()
)
```

### å·¥å…·è°ƒç”¨ï¼ˆå‡½æ•°è°ƒç”¨ï¼‰

```python
from ai_lib_python import ToolDefinition

# å®šä¹‰å·¥å…·
weather_tool = ToolDefinition.from_function(
    name="get_weather",
    description="è·å–æŒ‡å®šåœ°ç‚¹çš„å½“å‰å¤©æ°”",
    parameters={
        "type": "object",
        "properties": {
            "location": {"type": "string", "description": "åŸå¸‚åç§°"},
            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
        },
        "required": ["location"]
    }
)

# åœ¨è¯·æ±‚ä¸­ä½¿ç”¨å·¥å…·
response = await (
    client.chat()
    .user("ä¸œäº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    .tools([weather_tool])
    .execute()
)

# æ£€æŸ¥å·¥å…·è°ƒç”¨
if response.tool_calls:
    for tool_call in response.tool_calls:
        print(f"è°ƒç”¨ {tool_call.function_name}: {tool_call.arguments}")
```

### å¤šæ¨¡æ€ï¼ˆå›¾åƒï¼‰

```python
from ai_lib_python import Message, ContentBlock

# é€šè¿‡ URL ä¼ å…¥å›¾åƒ
message = Message.user_with_image(
    "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ",
    image_url="https://example.com/image.jpg"
)

# é€šè¿‡ base64 ä¼ å…¥å›¾åƒ
with open("photo.jpg", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

message = Message(
    role=MessageRole.USER,
    content=[
        ContentBlock.text("æè¿°è¿™å¼ å›¾ç‰‡ï¼š"),
        ContentBlock.image_base64(image_data, "image/jpeg"),
    ]
)

response = await client.chat().messages([message]).execute()
```

### ç”Ÿäº§å°±ç»ªé…ç½®

```python
from ai_lib_python import AiClient

# å¯ç”¨æ‰€æœ‰å¼¹æ€§æ¨¡å¼
client = await (
    AiClient.builder()
    .model("openai/gpt-4o")
    .production_ready()  # å¯ç”¨é‡è¯•ã€é™æµã€ç†”æ–­
    .with_fallbacks(["anthropic/claude-3-5-sonnet"])
    .build()
)

# æ£€æŸ¥å¼¹æ€§çŠ¶æ€
print(f"ç†”æ–­çŠ¶æ€: {client.circuit_state}")
print(f"æ­£åœ¨å¤„ç†çš„è¯·æ±‚: {client.current_inflight}")
print(client.get_resilience_stats())
```

### è‡ªå®šä¹‰å¼¹æ€§é…ç½®

```python
from ai_lib_python import AiClient
from ai_lib_python.resilience import (
    RetryConfig,
    RateLimiterConfig,
    CircuitBreakerConfig,
)

client = await (
    AiClient.builder()
    .model("openai/gpt-4o")
    .with_retry(RetryConfig(
        max_retries=5,
        min_delay_ms=1000,
        max_delay_ms=30000,
    ))
    .with_rate_limit(RateLimiterConfig.from_rps(10))
    .with_circuit_breaker(CircuitBreakerConfig(
        failure_threshold=5,
        cooldown_seconds=30,
    ))
    .max_inflight(20)
    .build()
)
```

### ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
async with await AiClient.create("openai/gpt-4o") as client:
    response = await client.chat().user("ä½ å¥½ï¼").execute()
    print(response.content)
# å®¢æˆ·ç«¯è‡ªåŠ¨å…³é—­
```

### Token è®¡æ•°ä¸æˆæœ¬ä¼°ç®—

```python
from ai_lib_python.tokens import TokenCounter, estimate_cost, get_model_pricing

# è®¡ç®— Token æ•°é‡
counter = TokenCounter.for_model("gpt-4o")
token_count = counter.count("ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ")
print(f"Token æ•°é‡: {token_count}")

# è®¡ç®—æ¶ˆæ¯ Token æ•°
messages = [Message.user("ä½ å¥½ï¼"), Message.assistant("ä½ å¥½å‘€ï¼")]
total_tokens = counter.count_messages(messages)

# ä¼°ç®—æˆæœ¬
cost = estimate_cost(input_tokens=1000, output_tokens=500, model="gpt-4o")
print(f"é¢„ä¼°æˆæœ¬: ${cost.total_cost:.4f}")

# è·å–æ¨¡å‹å®šä»·ä¿¡æ¯
pricing = get_model_pricing("gpt-4o")
print(f"è¾“å…¥ä»·æ ¼: ${pricing.input_price_per_1k}/åƒ Token")
print(f"ä¸Šä¸‹æ–‡çª—å£: {pricing.context_window} Token")
```

### æŒ‡æ ‡ä¸é¥æµ‹

```python
from ai_lib_python.telemetry import (
    get_logger,
    MetricsCollector,
    MetricLabels,
    Tracer,
)

# ç»“æ„åŒ–æ—¥å¿—
logger = get_logger("my_app")
logger.info("è¯·æ±‚å¼€å§‹", model="gpt-4o", tokens=100)

# æŒ‡æ ‡æ”¶é›†
collector = MetricsCollector()
labels = MetricLabels(provider="openai", model="gpt-4o")
collector.record_request(labels, latency=0.5, status="success", tokens_in=100, tokens_out=50)

# è·å–æŒ‡æ ‡å¿«ç…§
snapshot = collector.get_snapshot()
print(f"æ€»è¯·æ±‚æ•°: {snapshot.total_requests}")
print(f"P99 å»¶è¿Ÿ: {snapshot.latency_p99_ms:.2f}ms")

# å¯¼å‡º Prometheus æ ¼å¼
prometheus_metrics = collector.to_prometheus()

# åˆ†å¸ƒå¼è¿½è¸ª
tracer = Tracer("my_service")
with tracer.span("api_call") as span:
    span.set_attribute("model", "gpt-4o")
    # ... æ‰§è¡Œæ“ä½œ
```

### æ‰¹é‡å¤„ç†

```python
from ai_lib_python.batch import BatchExecutor, BatchConfig

# å¹¶å‘æ‰§è¡Œå¤šä¸ªè¯·æ±‚
async def process_question(question: str) -> str:
    client = await AiClient.create("openai/gpt-4o")
    response = await client.chat().user(question).execute()
    await client.close()
    return response.content

questions = ["ä»€ä¹ˆæ˜¯ AIï¼Ÿ", "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ", "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ"]

executor = BatchExecutor(process_question, max_concurrent=5)
result = await executor.execute(questions)

print(f"æˆåŠŸ: {result.successful_count}")
print(f"å¤±è´¥: {result.failed_count}")
for answer in result.get_successful_results():
    print(answer)
```

### æ¨¡å‹è·¯ç”±ä¸é€‰æ‹©

```python
from ai_lib_python.routing import (
    ModelManager, ModelInfo, create_openai_models, create_anthropic_models,
    CostBasedSelector, QualityBasedSelector,
)

# åˆ›å»ºé¢„é…ç½®çš„æ¨¡å‹ç®¡ç†å™¨
manager = create_openai_models()
manager.merge(create_anthropic_models())

# æŒ‰èƒ½åŠ›ç­›é€‰æ¨¡å‹
code_models = manager.filter_by_capability("code_generation")
print(f"ä»£ç ç”Ÿæˆæ¨¡å‹: {[m.name for m in code_models]}")

# é€‰æ‹©æœ€ä¾¿å®œçš„æ¨¡å‹
selector = CostBasedSelector()
cheapest = selector.select(manager.list_models())
print(f"æœ€ä¾¿å®œ: {cheapest.name} @ ${cheapest.pricing.input_cost_per_1k}/åƒToken")

# é€‰æ‹©æœ€é«˜è´¨é‡çš„æ¨¡å‹
quality_selector = QualityBasedSelector()
best = quality_selector.select(manager.list_models())
print(f"æœ€é«˜è´¨é‡: {best.name}")

# æŒ‰ç”¨ä¾‹æ¨èæ¨¡å‹
recommended = manager.recommend_for("chat")
```

### æµå¼å–æ¶ˆ

```python
from ai_lib_python.client import create_cancel_pair, CancellableStream, CancelReason

async def cancellable_stream():
    client = await AiClient.create("openai/gpt-4o")
    
    # åˆ›å»ºå–æ¶ˆä»¤ç‰Œå’Œå¥æŸ„
    token, handle = create_cancel_pair()
    
    # å¯åŠ¨æ”¯æŒå–æ¶ˆçš„æµå¼å¤„ç†
    stream = client.chat().user("å†™ä¸€ä¸ªé•¿æ•…äº‹...").stream()
    cancellable = CancellableStream(stream, token)
    
    # åœ¨å¦ä¸€ä¸ªä»»åŠ¡ä¸­å¯ä»¥å–æ¶ˆ:
    # handle.cancel(CancelReason.USER_REQUEST)
    
    async for event in cancellable:
        if event.is_content_delta:
            print(event.as_content_delta.content, end="")
        
        # æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
        if token.is_cancelled:
            print("\n[å·²å–æ¶ˆ]")
            break
```

### ç”¨æˆ·åé¦ˆæ”¶é›†

```python
from ai_lib_python.telemetry import (
    RatingFeedback, ThumbsFeedback, ChoiceSelectionFeedback,
    InMemoryFeedbackSink, set_feedback_sink, report_feedback,
)

# è®¾ç½®åé¦ˆæ”¶é›†å™¨
sink = InMemoryFeedbackSink(max_events=1000)
set_feedback_sink(sink)

# æŠ¥å‘Šç”¨æˆ·åé¦ˆ
await report_feedback(RatingFeedback(
    request_id="req-123",
    rating=5,
    category="helpfulness",
    comment="å›å¤å¾ˆæ£’ï¼"
))

await report_feedback(ThumbsFeedback(
    request_id="req-456",
    is_positive=True
))

# æŠ¥å‘Šå¤šå€™é€‰é€‰æ‹©ï¼ˆç”¨äº A/B æµ‹è¯•ï¼‰
await report_feedback(ChoiceSelectionFeedback(
    request_id="req-789",
    chosen_index=0,
    rejected_indices=[1, 2],
    latency_to_select_ms=1500.0
))

# è·å–åé¦ˆè®°å½•
all_feedback = sink.get_events()
request_feedback = sink.get_events_by_request("req-123")
```

### å‘é‡åµŒå…¥

```python
from ai_lib_python.embeddings import (
    EmbeddingClient, cosine_similarity, find_most_similar
)

# åˆ›å»ºåµŒå…¥å®¢æˆ·ç«¯
client = await EmbeddingClient.create("openai/text-embedding-3-small")

# ç”ŸæˆåµŒå…¥å‘é‡
response = await client.embed("ä½ å¥½ï¼Œä¸–ç•Œï¼")
embedding = response.first.vector
print(f"ç»´åº¦: {len(embedding)}")

# æ‰¹é‡ç”ŸæˆåµŒå…¥
texts = ["ä½ å¥½", "ä¸–ç•Œ", "Python", "AI"]
response = await client.embed_batch(texts)

# æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„
query = response.embeddings[0].vector
candidates = [e.vector for e in response.embeddings[1:]]
results = find_most_similar(query, candidates, top_k=2)
for idx, score in results:
    print(f"æ–‡æœ¬ '{texts[idx+1]}' ç›¸ä¼¼åº¦: {score:.4f}")

await client.close()
```

### å“åº”ç¼“å­˜

```python
from ai_lib_python.cache import CacheManager, CacheConfig, MemoryCache

# åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
cache = CacheManager(
    config=CacheConfig(default_ttl_seconds=3600),
    backend=MemoryCache(max_size=1000)
)

# ç¼“å­˜å“åº”
key = cache.generate_key(model="gpt-4o", messages=messages)

# å…ˆæ£€æŸ¥ç¼“å­˜
cached = await cache.get(key)
if cached:
    print("ç¼“å­˜å‘½ä¸­ï¼")
    response = cached
else:
    response = await client.chat().messages(messages).execute()
    await cache.set(key, response)

# è·å–ç¼“å­˜ç»Ÿè®¡
stats = cache.stats()
print(f"å‘½ä¸­ç‡: {stats.hit_ratio:.2%}")
```

### æ’ä»¶ç³»ç»Ÿ

```python
from ai_lib_python.plugins import (
    Plugin, PluginContext, PluginRegistry, HookType, HookManager
)

# åˆ›å»ºè‡ªå®šä¹‰æ’ä»¶
class LoggingPlugin(Plugin):
    def name(self) -> str:
        return "logging"
    
    async def on_before_request(self, ctx: PluginContext) -> None:
        print(f"è¯·æ±‚å‘å¾€ {ctx.model}: {ctx.request}")
    
    async def on_after_response(self, ctx: PluginContext) -> None:
        print(f"æ”¶åˆ°å“åº”: {ctx.response}")

# æ³¨å†Œæ’ä»¶
registry = PluginRegistry()
await registry.register(LoggingPlugin())

# ä½¿ç”¨é’©å­è¿›è¡Œç»†ç²’åº¦æ§åˆ¶
hooks = HookManager()
hooks.register(HookType.BEFORE_REQUEST, "log", lambda ctx: print(f"å¼€å§‹ {ctx.model}"))

# è§¦å‘é’©å­
ctx = PluginContext(model="gpt-4o", request={"messages": [...]})
await registry.trigger_before_request(ctx)
```

### æ‰¹é‡å¤„ç†

ä½¿ç”¨å¹¶å‘æ§åˆ¶çš„æ‰¹é‡æ‰§è¡Œï¼š

```python
from ai_lib_python.batch import BatchExecutor, BatchConfig

# å¹¶å‘æ‰§è¡Œå¤šä¸ªè¯·æ±‚
async def process_question(question: str) -> str:
    client = await AiClient.create("openai/gpt-4o")
    response = await client.chat().user(question).execute()
    await client.close()
    return response.content

questions = ["ä»€ä¹ˆæ˜¯ AIï¼Ÿ", "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ", "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ"]

executor = BatchExecutor(process_question, max_concurrent=5)
result = await executor.execute(questions)

print(f"æˆåŠŸ: {result.successful_count}")
print(f"å¤±è´¥: {result.failed_count}")
for answer in result.get_successful_results():
    print(answer)
```

## ğŸ¨ åè®®é©±åŠ¨æ¶æ„

æ²¡æœ‰ `match provider` è¯­å¥ã€‚æ‰€æœ‰é€»è¾‘éƒ½ä»åè®®é…ç½®æ´¾ç”Ÿï¼š

```python
# ç®¡é“ä»åè®®æ¸…å•åŠ¨æ€æ„å»º
pipeline = Pipeline.from_manifest(manifest)

# ç®—å­é€šè¿‡æ¸…å•ï¼ˆYAML/JSONï¼‰é…ç½®ï¼Œè€Œéç¡¬ç¼–ç 
# æ·»åŠ æ–°æä¾›å•†æ— éœ€ä¿®æ”¹ä»£ç 
```

### çƒ­é‡è½½

å¯ä»¥åœ¨è¿è¡Œæ—¶æ›´æ–°åè®®é…ç½®ï¼š

```python
from ai_lib_python.protocol import ProtocolLoader

loader = ProtocolLoader(hot_reload=True)
# åè®®æ›´æ”¹ä¼šè‡ªåŠ¨è¢«æ£€æµ‹å’Œåº”ç”¨
```

## æ”¯æŒçš„æä¾›å•†

| æä¾›å•† | æ¨¡å‹ | æµå¼ | å·¥å…·è°ƒç”¨ | è§†è§‰ |
|--------|------|------|----------|------|
| OpenAI | GPT-4o, GPT-4, GPT-3.5 | âœ… | âœ… | âœ… |
| Anthropic | Claude 3.5, Claude 3 | âœ… | âœ… | âœ… |
| Google | Gemini Pro, Gemini Flash | âœ… | âœ… | âœ… |
| DeepSeek | DeepSeek Chat, Coder | âœ… | âœ… | âŒ |
| é€šä¹‰åƒé—® | Qwen2.5, Qwen-Max | âœ… | âœ… | âœ… |
| Groq | Llama, Mixtral | âœ… | âœ… | âŒ |
| Mistral | Mistral Large, Medium | âœ… | âœ… | âŒ |

## API å‚è€ƒ

### æ ¸å¿ƒç±»

- **`AiClient`**: AI æ¨¡å‹äº¤äº’çš„ä¸»å…¥å£
- **`Message`**: è¡¨ç¤ºèŠå¤©æ¶ˆæ¯ï¼ŒåŒ…å«è§’è‰²å’Œå†…å®¹
- **`ContentBlock`**: å¤šæ¨¡æ€æ¶ˆæ¯çš„å†…å®¹å—
- **`ToolDefinition`**: å‡½æ•°è°ƒç”¨çš„å·¥å…·/å‡½æ•°å®šä¹‰
- **`StreamingEvent`**: æµå¼å“åº”çš„äº‹ä»¶

### å¼¹æ€§ç±»

- **`RetryPolicy`**: å¸¦æŠ–åŠ¨çš„æŒ‡æ•°é€€é¿é‡è¯•
- **`RateLimiter`**: ä»¤ç‰Œæ¡¶é™æµ
- **`CircuitBreaker`**: ç†”æ–­å™¨æ¨¡å¼
- **`Backpressure`**: å¹¶å‘é™åˆ¶
- **`FallbackChain`**: å¤šç›®æ ‡é™çº§
- **`PreflightChecker`**: ç»Ÿä¸€çš„è¯·æ±‚é¢„æ£€
- **`SignalsSnapshot`**: è¿è¡Œæ—¶çŠ¶æ€èšåˆ

### è·¯ç”±ç±»

- **`ModelManager`**: é›†ä¸­å¼æ¨¡å‹ç®¡ç†
- **`ModelInfo`**: å¸¦èƒ½åŠ›ä¿¡æ¯çš„æ¨¡å‹å®šä¹‰
- **`ModelArray`**: ç«¯ç‚¹é—´è´Ÿè½½å‡è¡¡
- **`ModelSelectionStrategy`**: é€‰æ‹©ç­–ç•¥ï¼ˆæˆæœ¬ã€è´¨é‡ã€æ€§èƒ½ç­‰ï¼‰

### é¥æµ‹ç±»

- **`AiLibLogger`**: å¸¦æ•æ„Ÿæ•°æ®è„±æ•çš„ç»“æ„åŒ–æ—¥å¿—
- **`MetricsCollector`**: è¯·æ±‚æŒ‡æ ‡æ”¶é›†
- **`Tracer`**: åˆ†å¸ƒå¼è¿½è¸ª
- **`HealthChecker`**: å¥åº·ç›‘æ§
- **`FeedbackSink`**: ç”¨æˆ·åé¦ˆæ”¶é›†

### åµŒå…¥ç±»

- **`EmbeddingClient`**: åµŒå…¥å‘é‡ç”Ÿæˆå®¢æˆ·ç«¯
- **`Embedding`**: å•ä¸ªåµŒå…¥ç»“æœ
- **`EmbeddingResponse`**: å¸¦ä½¿ç”¨ç»Ÿè®¡çš„å“åº”

### Token ç±»

- **`TokenCounter`**: Token è®¡æ•°æ¥å£
- **`CostEstimate`**: æˆæœ¬ä¼°ç®—ç»“æœ
- **`ModelPricing`**: æ¨¡å‹å®šä»·ä¿¡æ¯

### ç¼“å­˜ç±»

- **`CacheManager`**: é«˜çº§ç¼“å­˜ç®¡ç†
- **`CacheBackend`**: ç¼“å­˜åç«¯æ¥å£ï¼ˆå†…å­˜ã€ç£ç›˜ã€ç©ºï¼‰
- **`CacheKeyGenerator`**: ç¡®å®šæ€§é”®ç”Ÿæˆ

### æ‰¹å¤„ç†ç±»

- **`BatchCollector`**: è¯·æ±‚åˆ†ç»„
- **`BatchExecutor`**: å¹¶è¡Œæ‰§è¡Œ

### æ’ä»¶ç±»

- **`Plugin`**: æ’ä»¶åŸºç±»
- **`PluginRegistry`**: æ’ä»¶ç®¡ç†
- **`HookManager`**: äº‹ä»¶é©±åŠ¨é’©å­
- **`Middleware`**: è¯·æ±‚/å“åº”é“¾

### ä¼ è¾“ç±»

- **`ConnectionPool`**: HTTP è¿æ¥æ± 
- **`PoolConfig`**: è¿æ¥æ± é…ç½®

### å–æ¶ˆç±»

- **`CancelToken`**: åä½œå¼å–æ¶ˆä»¤ç‰Œ
- **`CancelHandle`**: å…¬å…±å–æ¶ˆæ¥å£
- **`CancellableStream`**: å¯å–æ¶ˆçš„å¼‚æ­¥è¿­ä»£å™¨

### é”™è¯¯ç±»

- **`AiLibError`**: åŸºç¡€é”™è¯¯ç±»
- **`ProtocolError`**: åè®®åŠ è½½/éªŒè¯é”™è¯¯
- **`TransportError`**: HTTP ä¼ è¾“é”™è¯¯
- **`RemoteError`**: æä¾›å•†è¿”å›çš„ API é”™è¯¯

## æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AiClient                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ChatRequest â”‚  â”‚  Resilience â”‚  â”‚    Protocol         â”‚  â”‚
â”‚  â”‚   Builder   â”‚  â”‚  Executor   â”‚  â”‚    Loader           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                   â”‚
          â–¼                â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HttpTransport â”‚ â”‚   Pipeline   â”‚ â”‚  ProtocolManifest   â”‚
â”‚   (httpx)       â”‚ â”‚   (è§£ç â†’    â”‚ â”‚  (YAML/JSON)        â”‚
â”‚                 â”‚ â”‚   é€‰æ‹©â†’     â”‚ â”‚                     â”‚
â”‚                 â”‚ â”‚   æ˜ å°„)     â”‚ â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª å¼€å‘

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/hiddenpath/ai-lib-python.git
cd ai-lib-python

# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œæµ‹è¯•
pytest

# è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=src/ai_lib_python

# ç±»å‹æ£€æŸ¥
mypy src

# ä»£ç æ£€æŸ¥
ruff check src tests

# æ ¼å¼åŒ–ä»£ç 
ruff format src tests
```

## é¡¹ç›®ç»“æ„

```
ai-lib-python/
â”œâ”€â”€ src/ai_lib_python/
â”‚   â”œâ”€â”€ __init__.py         # åŒ…å¯¼å‡º
â”‚   â”œâ”€â”€ types/              # ç±»å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ message.py      # Message, ContentBlock
â”‚   â”‚   â”œâ”€â”€ tool.py         # ToolDefinition, ToolCall
â”‚   â”‚   â””â”€â”€ events.py       # StreamingEvent ç±»å‹
â”‚   â”œâ”€â”€ protocol/           # åè®®å±‚
â”‚   â”‚   â”œâ”€â”€ manifest.py     # ProtocolManifest æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ loader.py       # åè®®åŠ è½½
â”‚   â”‚   â””â”€â”€ validator.py    # Schema éªŒè¯ï¼ˆ+ ç‰ˆæœ¬/æµå¼æ£€æŸ¥ï¼‰
â”‚   â”œâ”€â”€ transport/          # HTTP ä¼ è¾“
â”‚   â”‚   â”œâ”€â”€ http.py         # HttpTransport
â”‚   â”‚   â”œâ”€â”€ auth.py         # API å¯†é’¥è§£æ
â”‚   â”‚   â””â”€â”€ pool.py         # ConnectionPool
â”‚   â”œâ”€â”€ pipeline/           # æµå¤„ç†
â”‚   â”‚   â”œâ”€â”€ decode.py       # SSE/NDJSON è§£ç å™¨
â”‚   â”‚   â”œâ”€â”€ select.py       # JSONPath é€‰æ‹©å™¨
â”‚   â”‚   â”œâ”€â”€ accumulate.py   # å·¥å…·è°ƒç”¨ç´¯ç§¯å™¨
â”‚   â”‚   â”œâ”€â”€ event_map.py    # äº‹ä»¶æ˜ å°„å™¨
â”‚   â”‚   â””â”€â”€ fan_out.py      # FanOut, Replicate, Split å˜æ¢
â”‚   â”œâ”€â”€ resilience/         # å¼¹æ€§æ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ retry.py        # é‡è¯•ç­–ç•¥
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py # é™æµå™¨
â”‚   â”‚   â”œâ”€â”€ circuit_breaker.py  # ç†”æ–­å™¨
â”‚   â”‚   â”œâ”€â”€ backpressure.py # èƒŒå‹æ§åˆ¶
â”‚   â”‚   â”œâ”€â”€ fallback.py     # é™çº§é“¾
â”‚   â”‚   â”œâ”€â”€ executor.py     # å¼¹æ€§æ‰§è¡Œå™¨
â”‚   â”‚   â”œâ”€â”€ signals.py      # SignalsSnapshot
â”‚   â”‚   â””â”€â”€ preflight.py    # PreflightChecker
â”‚   â”œâ”€â”€ routing/            # æ¨¡å‹è·¯ç”±ä¸è´Ÿè½½å‡è¡¡
â”‚   â”‚   â”œâ”€â”€ models.py       # ModelInfo, ModelCapabilities
â”‚   â”‚   â”œâ”€â”€ strategies.py   # é€‰æ‹©ç­–ç•¥
â”‚   â”‚   â”œâ”€â”€ manager.py      # ModelManager
â”‚   â”‚   â””â”€â”€ array.py        # ModelArrayï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
â”‚   â”œâ”€â”€ client/             # ç”¨æˆ· API
â”‚   â”‚   â”œâ”€â”€ core.py         # AiClient
â”‚   â”‚   â”œâ”€â”€ builder.py      # æ„å»ºå™¨
â”‚   â”‚   â”œâ”€â”€ response.py     # ChatResponse
â”‚   â”‚   â””â”€â”€ cancel.py       # CancelToken, CancellableStream
â”‚   â”œâ”€â”€ embeddings/         # åµŒå…¥æ”¯æŒ
â”‚   â”‚   â”œâ”€â”€ client.py       # EmbeddingClient
â”‚   â”‚   â”œâ”€â”€ types.py        # Embedding, EmbeddingRequest
â”‚   â”‚   â””â”€â”€ vectors.py      # å‘é‡è¿ç®—
â”‚   â”œâ”€â”€ cache/              # å“åº”ç¼“å­˜
â”‚   â”‚   â”œâ”€â”€ manager.py      # CacheManager
â”‚   â”‚   â”œâ”€â”€ backend.py      # MemoryCache, DiskCache
â”‚   â”‚   â””â”€â”€ key.py          # CacheKeyGenerator
â”‚   â”œâ”€â”€ tokens/             # Token è®¡æ•°
â”‚   â”‚   â”œâ”€â”€ counter.py      # TokenCounter, TiktokenCounter
â”‚   â”‚   â””â”€â”€ pricing.py      # ModelPricing, CostEstimate
â”‚   â”œâ”€â”€ telemetry/          # å¯è§‚æµ‹æ€§
â”‚   â”‚   â”œâ”€â”€ logging.py      # AiLibLogger
â”‚   â”‚   â”œâ”€â”€ metrics.py      # MetricsCollector
â”‚   â”‚   â”œâ”€â”€ tracing.py      # Tracer
â”‚   â”‚   â”œâ”€â”€ health.py       # HealthChecker
â”‚   â”‚   â””â”€â”€ feedback.py     # åé¦ˆç±»å‹å’Œæ¥æ”¶å™¨
â”‚   â”œâ”€â”€ batch/              # è¯·æ±‚æ‰¹å¤„ç†
â”‚   â”‚   â”œâ”€â”€ collector.py    # BatchCollector
â”‚   â”‚   â””â”€â”€ executor.py     # BatchExecutor
â”‚   â”œâ”€â”€ plugins/            # æ’ä»¶ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ base.py         # Plugin åŸºç±»
â”‚   â”‚   â”œâ”€â”€ registry.py     # PluginRegistry
â”‚   â”‚   â”œâ”€â”€ hooks.py        # HookManager
â”‚   â”‚   â””â”€â”€ middleware.py   # Middleware é“¾
â”‚   â”œâ”€â”€ structured/         # ç»“æ„åŒ–è¾“å‡º
â”‚   â”‚   â”œâ”€â”€ json_mode.py    # JsonModeConfig
â”‚   â”‚   â”œâ”€â”€ schema.py       # SchemaGenerator
â”‚   â”‚   â””â”€â”€ validator.py    # OutputValidator
â”‚   â”œâ”€â”€ utils/              # å·¥å…·ç±»
â”‚   â”‚   â””â”€â”€ tool_call_assembler.py  # ToolCallAssembler
â”‚   â””â”€â”€ errors/             # é”™è¯¯å±‚æ¬¡
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/               # å•å…ƒæµ‹è¯•
â”‚   â””â”€â”€ integration/        # é›†æˆæµ‹è¯•
â”œâ”€â”€ docs/                   # æ–‡æ¡£
â”œâ”€â”€ examples/               # ç¤ºä¾‹è„šæœ¬
â””â”€â”€ pyproject.toml
```

## ğŸ“– ç›¸å…³é¡¹ç›®

- [AI-Protocol](https://github.com/hiddenpath/ai-protocol) - åè®®è§„èŒƒï¼ˆv1.5ï¼‰
- [ai-lib-rust](https://github.com/hiddenpath/ai-lib-rust) - Rust è¿è¡Œæ—¶å®ç°

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·ç¡®ä¿ï¼š

1. æ‰€æœ‰åè®®é…ç½®éµå¾ª AI-Protocol v1.5 è§„èŒƒ
2. æ–°åŠŸèƒ½æœ‰é€‚å½“çš„æ–‡æ¡£å’Œç¤ºä¾‹
3. æ–°åŠŸèƒ½åŒ…å«æµ‹è¯•
4. ä»£ç éµå¾ª Python æœ€ä½³å®è·µï¼ˆPEP 8ï¼‰å¹¶é€šè¿‡ `ruff check` æ£€æŸ¥

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ä»¥ä¸‹ä»»ä¸€è®¸å¯è¯ï¼š

- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) æˆ– http://www.apache.org/licenses/LICENSE-2.0)
- MIT License ([LICENSE-MIT](LICENSE-MIT) æˆ– http://opensource.org/licenses/MIT)

ç”±æ‚¨é€‰æ‹©ã€‚

---

**ai-lib-python** - åè®®ä¸ Python ä¼˜é›…çš„ç›¸é‡ã€‚ğŸâœ¨
